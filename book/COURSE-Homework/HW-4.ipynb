{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 4 — Build Your Own Word Frequency Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: Your name here  \n",
    "Net ID: Your ID here\n",
    "(Double-click this cell to type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "In this homework assignment, you will be building your own word frequency counter function, which will return the top 15 most frequent words in any text file. You will test this word frequency counter on two sample files: F. Scott Fitzgerald's *The Great Gatsby* and Trump's tweets from 2016 to early 2020.\n",
    "\n",
    "Your word frequency counter must split the text into individual words, make the words lowercase, remove stopwords, count words by frequency, and then return the top 15 most frequent words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment will slowly build toward the following end goal.  \n",
    "\n",
    "**End goal:** Calculate top 15 most frequent words in a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('john', 45),\n",
       " ('one', 33),\n",
       " ('said', 30),\n",
       " ('would', 27),\n",
       " ('get', 24),\n",
       " ('see', 24),\n",
       " ('room', 24),\n",
       " ('pattern', 24),\n",
       " ('paper', 23),\n",
       " ('like', 21),\n",
       " ('little', 20),\n",
       " ('much', 16),\n",
       " ('good', 16),\n",
       " ('think', 16),\n",
       " ('well', 15)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open('The-Yellow-Wallpaper_Charlotte-Perkins-Gilman.txt').read() \n",
    "tokenize_and_count(text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we tackle an entire novel, we will build up our code by practicing on a single line of text from *The Great Gatsby*. The first thing we need to do is figure out how to make the text lowercase and split it into individual words.\n",
    "\n",
    "**1:** Make all the words in `first_line` lowercase and then assign this string to the variable `first_line_lowercase`. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_line = \"In my younger and more vulnerable years my father gave me some advice that I’ve been turning over in my mind ever since.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_line_lowercase ... #Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in my younger and more vulnerable years my father gave me some advice that i’ve been turning over in my mind ever since.\n"
     ]
    }
   ],
   "source": [
    "print(first_line_lowercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2:** Now split the string `first_line_lowercase` into a list of individual words (without puncutation) by using the [regular expressions](https://info1350.github.io/Intro-CA-SP21/04-Data-Collection/03-Web-Scraping-Part2.html#regular-expressions) `re.split()` method. Then assign this list to the variable `split_words`. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sample',\n",
       " 'string',\n",
       " 'feel',\n",
       " 'free',\n",
       " 'to',\n",
       " 'copy',\n",
       " 'this',\n",
       " 'construction',\n",
       " '']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.split('\\W+', \"This is a sample string; feel free to copy this construction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_words ... #Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'my', 'younger', 'and', 'more', 'vulnerable', 'years', 'my', 'father', 'gave', 'me', 'some', 'advice', 'that', 'i', 've', 'been', 'turning', 'over', 'in', 'my', 'mind', 'ever', 'since', '']\n"
     ]
    }
   ],
   "source": [
    "print(split_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3:** Next we want to filter out \"stopwords\" — commonly used words that are removed as a pre-processing step in text analysis that so we can focus on more meaningful words.\n",
    "\n",
    "Make a new list called `clean_words` that consists of all the words in `split_words` if they do not appear in the list `stopwords`. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    " 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    " 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    " 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    " 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    " 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    " 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    " 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    " 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    " 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    " 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    " 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 've', 'll', 'amp', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words = []\n",
    "#Your Code Here\n",
    "    #Your Code Here\n",
    "        #Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['younger', 'vulnerable', 'years', 'father', 'gave', 'advice', 'turning', 'mind', 'ever', 'since']\n"
     ]
    }
   ],
   "source": [
    "print(clean_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4:** Let's bundle the first three questions together into a single function called `tokenize`. This is a vocabulary term in text analysis that means to split text into individual units or \"tokens.\"\n",
    "\n",
    "Write a function called `tokenize()` that takes in any string, makes the string lowercase, splits the string into individual words (without puncutation), removes any stopwords, and finally returns the remaining words. (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(any_text):\n",
    "    #Your Code Here\n",
    "    #Your Code Here\n",
    "    #Your Code Here\n",
    "    #Your Code Here\n",
    "        #Your Code Here\n",
    "            #Your Code Here\n",
    "    #Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['younger',\n",
       " 'vulnerable',\n",
       " 'years',\n",
       " 'father',\n",
       " 'gave',\n",
       " 'advice',\n",
       " 'turning',\n",
       " 'mind',\n",
       " 'ever',\n",
       " 'since']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(first_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4:** Let's add one more wrinkle to the previous function before we run it on *The Great Gatsby*.\n",
    "\n",
    "Make a function called `tokenize_and_count()` that tokenizes the words in any string (lowercases, splits, removes stopwrods) and then prints out the top 15 most frequent words. See below for a refresher on the `Counter` function. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kiwi', 3), ('apple', 2)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruits = ['apple', 'banana', 'kiwi', 'apple', 'kumquat', 'orange', 'orange', 'kiwi', 'kiwi']\n",
    "Counter(fruits).most_common(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_count(any_text):\n",
    "    #Your Code Here\n",
    "    #Your Code Here\n",
    "    #Your Code Here\n",
    "    #Your Code Here\n",
    "        #Your Code Here\n",
    "            #Your Code Here\n",
    "    #Your Code Here  \n",
    "    return most_frequent_words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5:** Finally, open and read the file \"The-Great-Gatsby_Fitzgerald.txt\" and assign to the variable `gatsby_text`. Then print out the top 15 words from *The Great Gatsby* with your `tokenize_and_count()` function.  (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gatsby_text = open('The-Great-Gatsby_Fitzgerald.txt', encoding='utf-8').#Your Code Here\n",
    "tokenize_and_count(gatsby_text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6:** Then open and read the file \"Trump-Tweets_2016-2020.txt\" and assign to the variable `trump_text`. Then print out the top 15 words from Trump's tweets with your `tokenize_and_count()` function.  (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_text = open('Trump-Tweets_2016-2020.txt').#Your Code Here\n",
    "tokenize_and_count(trump_text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus!** For 2 bonus points, rewrite the `tokenize_and_count()` function so that it accepts a keyword argument for the number of most frequent words to return. Then run it on Trump's tweets and configure it to return 30 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def #Your Code Here\n",
    "    #Your Code Here\n",
    "    #Your Code Here\n",
    "    #Your Code Here\n",
    "    #Your Code Here\n",
    "        #Your Code Here\n",
    "            #Your Code Here\n",
    "    #Your Code Here  \n",
    "    return most_frequent_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('Trump-Tweets_2016-2020.txt').#Your Code Here\n",
    "#Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When You're Finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're finished, you should:\n",
    "\n",
    "1. Save your Jupyter notebook with your last name. To save your notebook, you can select File -> Save Notebook or File -> Save Notebook As...\n",
    "\n",
    "2. Upload the Jupyter notebook file (.ipynb) to Gradescope. You can do so by locating your file on your computer and then dragging and dropping it into Gradescope.\n",
    "\n",
    "![](../images/submit-hw-gradescope.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
